\newcommand{\stateSpace}{\ensuremath{S_{V, \mu}}}
\begin{comment}
	This chapter is mostly focused on trace abstraction $\rightarrow$  It introduces the reader to the concept of trace abstraction. \\
	- Introduce logic, logical variables, terms, formulas, transition formulas with primed and unprimed variables, programs, program states, loops $\rightarrow$  then program-, error traces, feasible and infeasible counterexamples, CFGs, interpolants. \\ - From intuitive to true definitions. \\
	Here the running example from the introduction gets dissected to illustrate the definitions. \\ 
	Further the problems loops can cause are introduced, followed by a definition of loop summaries $\rightarrow$ introduction reflexive transitive closure of a formula 
	15 pages
\end{comment}

This chapter will introduce mathematical background, our understanding and notation of logic and formulas, programs, control-flow, and other needed background definitions. Furthermore, we will give an overview of the trace abstraction \cite{10.1007/978-3-642-03237-0_7} counterexample-guided abstraction refinement scheme used in \ultimate.

\section{Mathematical Background}
In this thesis we utilize various concepts of linear algebra, such as vector spaces, vector space bases, and more. This section will introduce important notations and definitions.
We define a \textsl{vector} as $\vec{x} = x_1, \ldots, x_n$ that consists of $n$ entries, where $n$ is called the dimension of the vector. The vector containing only zeroes is denoted as $\vec{0}$, the vector containing only ones by $\vec{1}$. \\ We denote an $m \times n$ \textsl{matrix} $S$ as
\begin{equation*}
	S = \begin{bmatrix}
		s_{1 ,1} & \ldots & s_{1, n} \\
		\vdots & \ddots & \vdots \\
		s_{m ,1} & \ldots & s_{m, n} \\
	\end{bmatrix}
\end{equation*}
with entries $s_{1, 1}, \ldots, s_{m, n}$. The integers$m$ and $n$ denote the number of columns and the number of rows respectively. A vector can be understood as a matrix of dimensions $1 \times n$, called row vector, or as a matrix of dimension $n \times 1$, called column vector. \\
We define the following operations on vectors and matrices.
\begin{mydef}[Scalar Multiplication]
	Given a scalar $a$ and an $m \times n$ matrix $S$
	\begin{equation*}
		S = 
		\begin{bmatrix}
			a_{11} & \ldots & a_{1n} \\
			\vdots & \ddots & \vdots \\
			a_{m1} & \ldots & a_{mn} \\
		\end{bmatrix}
		\end{equation*} define the scalar multiplication $aS$ is defined as the $m \times n$ matrix:
		\begin{equation*}
		aS =
		\begin{bmatrix}
			c_{11} & \ldots & c_{1n} \\
			\vdots & \ddots & \vdots \\
			c_{m1} & \ldots & c_{mn} \\
		\end{bmatrix}
	\end{equation*} 
\\
	consisting of entries $c_{ij} =  a \cdot a_{ij}$ for $1 \leq i \leq m$ and $1 \leq j \leq n$.
\end{mydef}
\begin{mydef}[Matrix Multiplication]
	Given the $m \times n$ matrix $A$ and $n \times p$ matrix $B$:
	\begin{minipage}{0.4\linewidth}
		\begin{equation*}
		A = 
		\begin{bmatrix}
			a_{1 ,1} & \ldots & a_{1, n} \\
			\vdots & \ddots & \vdots \\
			a_{m ,1} & \ldots & a_{m, n} \\
		\end{bmatrix}
	\end{equation*}
\end{minipage}	
\begin{minipage}[b]{0.59\linewidth}
	\begin{equation*}
		B = 
		\begin{bmatrix}
			b_{1 ,1} & \ldots & b_{1, p} \\
			\vdots & \ddots & \vdots \\
			b_{n ,1} & \ldots & b_{n, p} \\
		\end{bmatrix}
	\end{equation*}
\end{minipage}
\\ \\
we define the matrix product $AB$ as the $m \times p$ matrix:
\begin{equation*}
		AB = 
		\begin{bmatrix}
			c_{11} & \ldots & c_{1p} \\
			\vdots & \ddots & \vdots \\
			c_{m1} & \ldots & c_{mp} \\
		\end{bmatrix}
	\end{equation*} 
	consisting of entries $c_{i,j} = \sum_{k=1}^n a_{ik}\cdot b_{kj}$ for $1 \leq i \leq m$ and $1 \leq j \leq p$.
	\end{mydef}

\begin{mydef}[Hadamard Product]
	Given matrices of same dimensions $m \times n$ $A$ and $B$:
	\begin{minipage}{0.4\linewidth}
		\begin{equation*}
			A = 
			\begin{bmatrix}
				a_{1 ,1} & \ldots & a_{1, n} \\
				\vdots & \ddots & \vdots \\
				a_{m ,1} & \ldots & a_{m, n} \\
			\end{bmatrix}
		\end{equation*}
	\end{minipage}	
	\begin{minipage}[b]{0.59\linewidth}
		\begin{equation*}
			B = 
			\begin{bmatrix}
				b_{1 ,1} & \ldots & b_{1, n} \\
				\vdots & \ddots & \vdots \\
				b_{m ,1} & \ldots & b_{m, n} \\
			\end{bmatrix}
		\end{equation*}
	\end{minipage}	
\\ \\
	we define the Hadamard product $A*B$ as a  $m \times n$ matrix:
	\begin{equation*}
	 A*B = 
	\begin{bmatrix}
		c_{1 ,1} & \ldots & c_{1, p} \\
		\vdots & \ddots & \vdots \\
		c_{m ,1} & \ldots & c_{m, p} \\
	\end{bmatrix}
	\end{equation*}
	With entries $c_{i,j} = a_{ij} \cdot b_{ij}$ for $1 \leq i \leq m$ and $1 \leq j \leq p$.
\end{mydef}
\begin{mydef}[Linear Independence]
	The sequence of vectors $\vec{x_1}, \ldots, \vec{x_n}$ of same dimension $d$, is linear independent, if and only if there exists no non-zero scalars $a_1, \ldots, a_d$ such that $a_1\vec{x_1} + \ldots + a_n\vec{x_n} = \vec{0}$.
\end{mydef}
\begin{mydef}[Vector Spaces]
	Given vectors $\vec{x_1}, \ldots, \vec{x_n}$, the set of vectors \\ $S = \{\vec{x_1}, \ldots, \vec{x_n}\}$ is called a vector space.
\end{mydef}
\begin{mydef}[Linear Combination]
	Given a vector space $S = \{\vec{x_1}, \ldots, \vec{x_n}\}$ and a sequence of scalars $a_1, \ldots, a_n$, a linear combination of vectors and scalars is the sum $a_1\vec{x_1} + \ldots + a_n\vec{n}$.
\end{mydef}
\begin{mydef}[Vector Space Basis]
	Given a vector space $S$, a set of vectors $B \subseteq S$ is a basis for vector space $S$ if its elements are linearly independent and every element of $S$ is a linear combination of elements of B.
\end{mydef}


\section{Logical Background}
To represent programs formally, we make use of first-order logic. This Chapter will introduce definitions and notations used throughout this thesis.

\subsection{Notation}
We utilize the standard logical notations: We represent boolean values as $\bot$, meaning \textsl{false}, and $\top$, meaning \textsl{true}. \\ Logical connectives are defined as:
\begin{itemize}
	\item Negation (\textsl{not}) denoted by: $\neg$
	\item Conjunction (\textsl{and}) denoted by: $\land$
	\item Disjunction (\textsl{or}) denoted by: $\lor$
	\item Implication (\textsl{if $\ldots$ then}) denoted by: $\rightarrow$
	\item Biconditional (\textsl{if and only if}) denoted by: $\leftrightarrow$
\end{itemize}
Formulas can be quantified, we define quantifiers as:
\begin{itemize}
	\item Existential quantification (\textsl{there exists}) denoted by: $\exists$
	\item Universal quantification (\textsl{for all}) denoted by: $\forall$
\end{itemize}

\subsection{Syntax}
We firstly introduce our first-order logic syntax.
Let $V = (\vocab{Var}, \vocab{Const}, \vocab{Fun}, \vocab{Pred})$ be a vocabulary consisting of the countable sets:
\begin{itemize}
	\item $\vocab{Var}$ containing all \textsl{variables}
	\item $\vocab{Const}$ containing all \textsl{constant symbols}
	\item $\vocab{Fun} $ containing all \textsl{function symbols}. Each symbol $f \in \vocab{Fun}$ where as a natural number $\geq 1$ called arity of $f$
 	\item $\vocab{Pred}$ containing all \textsl{predicate symbols}. Each symbol $p \in \vocab{Pred}$ has a natural number $\geq 0$ called arity of $p$
\end{itemize}
Assume we are given such a vocabulary, we can construct first-order logic terms using the symbols in the vocabulary.
\begin{mydef}[Term] 
	Given a vocabulary $V = (\vocab{Var}, \vocab{Const}, \vocab{Fun}, \vocab{Pred})$, we define terms inductively as follows:
	\begin{itemize}
		\item Every $x \in \vocab{Var}$ is a term.
		\item Every $c \in \vocab{Const}$ is a term.
		\item If $t_0, \ldots, t_n$ are terms and $f \in \vocab{Fun}$ is a function symbol with arity $n$, then $f(t_0, \ldots, t_n)$ is a term.
	\end{itemize}
\end{mydef}
Using first-order logic terms, we can introduce first-order logic formulas.
\begin{mydef}[Formula]
	Given vocabulary $V = (\vocab{Var}, \vocab{Const}, \vocab{Fun}, \vocab{Pred})$, first-order logic formulas are inductively defined as follows:
	\begin{itemize}
		\item $\bot$ is a formula.
		\item If  $t_0, \ldots, t_n$ are terms, and $p \in \vocab{Pred}$ is a predicate symbol with arity $n$, \\ then $p(t_0, \ldots, t_n)$ is a formula.
		\item If $\varphi$ is a formula, then $\neg \varphi$ is a formula.
		\item If $\varphi$ and $\psi$ are formulas, then $\varphi \land \psi$ are formulas.
		\item If $\varphi$ is a formula, and $x \in \vocab{Var}$ then $\exists x. \varphi$ is a formula.
	\end{itemize}
\end{mydef}
To give variables, constants, functions, and predicates concrete values, we can assign them a model.
\begin{mydef}[Model]
	Given vocabulary $V = (\vocab{Var}, \vocab{Const}, \vocab{Fun}, \vocab{Pred})$, a model $\mathcal{M} = (D, \interpret)$ is a tuple consisting of a nonempty set $D$, called interpretation domain, and an interpretation function \interpret that assigns constants, functions, and predicates over $D$ to symbols in $V$. $M$ has the following characteristics:
	\begin{itemize}
		\item The domain of \interpret is $\vocab{Const} \cup \vocab{Fun} \cup \vocab{Pred}$
		\item \interpret maps every constant symbol $c \in \vocab{Const}$ to an element in $D$
		\item \interpret maps every function symbol $f \in \vocab{Fun}$, with arity $n$, to a corresponding n-ary function with domain $D^n$ and range $D$
		\item \interpret maps every predicate symbol $p \in \vocab{Pred}$ with arity $n$ to an n-ary relation over the domain $D$
	\end{itemize}
\end{mydef}
Using a model, we can now assign concrete values to variables.
\begin{mydef}[Assignment of Variables]
	Given vocabulary \\ $V = (\vocab{Var}, \vocab{Const}, \vocab{Fun}, \vocab{Pred})$, and model $\mathcal{M} = (D, \interpret)$, an assignment of variable $v \in \vocab{Var}$ is a function $\rho: v \rightarrow D$, that maps each variable to a value in domain $D$.
\end{mydef}
Assume $f \in \vocab{Fun}$ is a function defined as $f: X \rightarrow Y$ with some domain $X$ and range $Y$. Let $x \in X$ and $y \in Y$, we use $f[x \mapsto y]$  to denote the function that maps all $\bar{x} \in X \backslash \{ x \}$ to $f(\bar{x})$ and $x$ to $y$.

\subsection{Semantics}
Assume we are given a vocabulary $V = (\vocab{Var}, \vocab{Const}, \vocab{Fun}, \vocab{Pred})$, we know how to assign values using models and variable assignments. The task now is to understand how to interpret them. This section serves to introduce semantics of fist-order logic.
\begin{mydef}[Evaluation of Terms]
	Let $V = (\vocab{Var}, \vocab{Const}, \vocab{Fun}, \vocab{Pred})$ be a vocabulary, $\mathcal{M} = (D, \interpret)$ a model, and $\rho$ a variable assignment. The evaluation of terms is a function $\eval{\cdot}$ that is inductively defined as:
	\begin{itemize}
		\item For each $v \in \vocab{Var}$, $\eval{v} = \rho(v)$
		\item For each $c \in \vocab{Const}$, $\eval{c} = \interpret(c)$
		\item If $t_0, \ldots, t_n$ are terms, $f \in \vocab{Fun}$, with f having arity $n$ then \\ $\eval{f(t_0, \cdots, t_n)} = \interpret(f)(\eval{t_0}, \ldots, \eval{t_n})$
	\end{itemize}
\end{mydef}
Furthermore, given a formula $\varphi$, we denote the substitution of variable $v$ by $x$ as:
\begin{equation*}
	\varphi[v \mapsto x]
\end{equation*}
From the evaluation of terms we can derive the evaluation of formulas, which decides whether a formula is $\textit{\textbf{true}}$ or \textit{\textbf{false}}.

\begin{mydef}[Evaluation of Formulas]
		Let $V = (\vocab{Var}, \vocab{Const}, \vocab{Fun}, \vocab{Pred})$ be a vocabulary, $\mathcal{M} = (D, \interpret)$ a model, a variable assignment $\rho$, and $\varphi_0,\  \varphi_1$ being first-order logic formulas over $V$. The evaluation of formulas is a function $\eval{\cdot}$ that is inductively defined as: \\
		\begin{itemize}
			\item {\makebox[3.25cm]{$\eval{\bot} = \hfill$}} = \ \ \textbf{false}
			\item {\makebox[3.25cm]{$ \eval{p(t_0, \ldots, t_n)} \hfill$}} =
				$
				\begin{cases}
					\textbf{true}, & \text{for } (\eval{t_0}, \ldots, \eval{t_n}) \in \interpret(p)\\
					\textbf{false}, & \text{otherwise}
				\end{cases}
				$
			\item {\makebox[3.25cm]{$\eval{\neg \varphi_0} \hfill$}} =
				$
				\begin{cases}
					\textbf{true} & \text{for } \eval{\varphi_0} \text{ \textbf{false}}\\
					\textbf{false}, & \text{for } \eval{\varphi_0} \text{ \textbf{true}}\\
				\end{cases}
				$
			\item {\makebox[3.25cm]{$\eval{\varphi_0 \land \varphi_1} \hfill$}} =
				$
				\begin{cases}
					\textbf{true}, & \text{for } \eval{\varphi_0} \text{ \textbf{true} and } \eval{\varphi_1} \textbf{ true} \\
					\textbf{false}, & \text{otherwise} \\
				\end{cases}
				$
			\item {\makebox[3.25cm]{$\eval{\exists v. \varphi_0} \hfill$}} =
				$
				\begin{cases}
					\textbf{true}, & \text{if there exists } x \in D \text{ where } \eval{\varphi_0 [v \mapsto x]} \textbf{ true} \\
					\textbf{false}, & \text{otherwise} \\
				\end{cases}
				$
		\end{itemize}
\end{mydef}
A formula $\varphi$ is called satisfiable if there exists a model $\mathcal{M} = (D, \interpret)$ and a variable assignment $\rho$ such that $\eval{\varphi}$ is $\textit{\textbf{true}}$. A formula is valid if there is no model $\mathcal{M} = (D, \interpret)$, such $\eval{\varphi}$ is $\textit{\textbf{false}}$.
\section{Programs}
In this chapter we will introduce our understanding on how to model programs using first-order logic. We will begin with our syntax of programs, and then explain program statement semantics.

\subsection{Program Syntax}
Assume we are given the example program as seen in Figure \ref{codeNoAss}. We can see that the code contains various typed variables such as \texttt{x:int} and instructions over these variables. \\ These instructions use the following context-free grammar $\Sigma$ that is a derived and simplified version of the grammar of the intermediate verification language Boogie\cite{Boogie}.
\setlength{\grammarparsep}{20pt plus 1pt minus 1pt} % increase separation between rules
\setlength{\grammarindent}{12em} % increase separation between LHS/RHS
\begin{figure}[H]
	\input{fig/grammar.tex}
	\caption{Context-free grammar $\Sigma$ detailing program instructions.}
	\label{grmr}
\end{figure}
We call words derived from $\langle Stmt \rangle$ program statements, words derived from $\langle Expr \rangle$ expressions, and $v$ represents a program variable or a constant in a fitting model.
We define programs as follows:

\begin{mydef}[Programs]
	Given a set of variables $V$, a function $\mu: V \rightarrow \{ \mathbb{Z}, \mathbb{R}, \mathbb{B} \}$ that maps variables $v \in V$ to a domain, which is either the set of integers, the rational numbers, or boolean values $\mathbb{B} = \{\textbf{true}, \textbf{false}\}$, a program is a triple \prg, where $st$ is a derived word from $\Sigma$ representing the program instructions.
\end{mydef}
\subsection{Program Semantics}
We consider five kinds of program statements in the grammar $\Sigma$: An assumption over variables \texttt{assume}, an assignment to variables \texttt{:=}, a non deterministic assignment \texttt{havoc}, a program loop \texttt{while}, that repeats a statement until its loop guard, the $\langle WildcardExpr \rangle$, no longer holds, and a conditional branching, in form of an \texttt{if else} statement. With the first three: \st{assume $\langle Expr \rangle$ }, \st{v := $\langle Expr \rangle$}, and \st{havoc v}, being atomic statements, and \texttt{while} and \texttt{if else} being a compound of atomic statements.
We regard expressions $\langle Expr \rangle$ as logical formulas. Most program statements change the assignment of variables and with that changes the state a program is in. 
\begin{mydef}[Program State]
	Given a program \prg a program state is a function $\sigma: V \rightarrow \mu(V)$ that assigns each variable $v \in V$ a value $\mu(v)$ in its domain. We denote the set of all program states as \stateSpace.
\end{mydef}
We use fist-order logic formulas to detail sets of states.
\begin{mydef}[Sets of Program States]
	Given a first-order logic formula $\varphi$, the set of program states $\{ \varphi \}$ contains all program states $\sigma_i: V \rightarrow \mu_i(V)$ for which \\ $\eval{\varphi [V\mapsto \mu_i(V) ]}$ is \textit{\textbf{false}}.
\end{mydef}
The set of all program states $\stateSpace$ is defined as $\{\top\}$ and the empty set of program states as $\{\bot\}$.
Using program states, we can now set the semantics for the three atomic program statements.
\begin{mydef}[Program Statement Semantics]
		Given a program \prg, we define the semantics of program statements as a binary relation over program states $\sigma_1, \sigma_2$, whereas $\sigma_1$ denotes the program's state before the statement and $\sigma_2$ after. To model time steps, the variables in $\sigma_2$ are replaced by so called primed variables, meaning every variable $v \in V$ is replaced by $v'$. \\
	\begin{itemize}
		\item The semantics of the \textnormal{\texttt{assumption}} statement: \textnormal{\st{assume $\langle Expr \rangle$}} are defined as the relation: \\
		\begin{equation*}
			\{(\sigma_1, \sigma_2) \in \stateSpace \times \stateSpace\ |\ \eval{\langle Expr \rangle}\ \land\ \bigwedge\limits_{v \in V} v' = v  \text{ is true}\}
		\end{equation*}
		\item The semantics of the \textnormal{\texttt{assignment}} statement: \textnormal{\st{x := $\langle Expr \rangle$}} are defined as the relation: \\
		\begin{equation*}
			 \{(\sigma_1, \sigma_2) \in \stateSpace \times \stateSpace\ |\ \eval{x' = \langle Expr\rangle } \land\ \bigwedge\limits_{v \in V, v \not= x} v' = v 
			\text{ is true}\}
		\end{equation*}
		\item The semantics of the \textnormal{\texttt{havoc}} statement: \textnormal{\st{havoc x}} are defined as the relation: \\
		\begin{equation*}
			\{(\sigma_1, \sigma_2) \in \stateSpace \times \stateSpace\ |\ \bigwedge\limits_{v 	\in V, v \not= x} v' = v 
			\text{ is true} \}
		\end{equation*}
	\end{itemize}
\end{mydef}


\begin{comment}
	\begin{mydef}[Sets of Program States]
	To represent multiple program states we use first-order logic formulas. Given a first-order logic formula $\varphi$, defined over variables in $V$, we denote $\{\varphi\} = \{s \in \stateSpace | \eval{\varphi} \text{ where } \rho = s\}$
	\end{mydef}
\end{comment}
We use $(\sigma_1, \sigma_2) \in \st{st}$ to denote that the tuple $(\sigma_1, \sigma_2)$ is in the transition relation of $\st{st}$.
To model while and if else, we can concatenate these three atomic statements to form sequences of statements.
\begin{mydef}[Concatenation of Program Statements]
	Let \st{$s_1$} and \st{$s_2$} be atomic program statements and $\sigma_1$, $\sigma_2$ be program states, the semantics of the concatenation \st{$s_1;s_2$} are defined as:
	\begin{equation*}
		\{(\sigma_1, \sigma_2) \in \stateSpace \times \stateSpace\ |\ \exists \sigma_3 \in \stateSpace. (\sigma_1, \sigma_3) \in \text{\st{$s_1$}} \land (\sigma_3, \sigma_2) \in \text{\st{$s_2$}} \}
	\end{equation*}
\end{mydef}

The change from one program state to another is called a \textsl{transition}. To model transitions efficiently, we introduce transition formulas.
\begin{mydef}[Transition Formula]
		Given a program \prg with variable $v \in V$, a transition from program state $\sigma_1$ to state $\sigma_2$ is characterized by a transition formula defined over variables $V$ and $V'$ whereas $V'$ contains all variables found in $V$ but primed. The set $V$ characterizes the variables before the transition, e.g. the variable valuations in $\sigma_1$, in contrast to $V'$ which represents the variable valuations after the transition. A transition between two states is only possible, if and only if the transition formula is satisfiable for the given states. The number of variables in $V$ is called the dimension of the transitions formula.
\end{mydef}
 We specify for each of the atomic program statements the following \textsl{transition formulas}: 
 \begin{center}
	\st{assume $\langle Expr \rangle$}:
	\begin{align*}
		Expr \land \bigwedge\limits_{v \in V} v' = v
	\end{align*}
	\vspace*{0.5cm}

	\st{x := $\langle Expr \rangle$}:
	\begin{align*}
		x' = Expr \land \bigwedge\limits_{v \in V,\ v \not= x} v' = v 
	\end{align*}
	\vspace*{0.5cm}
	
	\st{havoc x}: 
	\begin{align*}
			\bigwedge\limits_{v \in V, v \not= x} v' = v 
	\end{align*}
 \end{center}
For brevity's sake we will use the notation \st{$\langle Expr \rangle$} for assumptions, and omit $\bigwedge\limits_{v \in V} v' = v$ in assumptions and assignments. \\
With transition formulas we can model state transitions using the strongest postcondition. The strongest postcondition applied on a program state $\sigma$, that satisfies a given first-order logic formula $\varphi$, and a transition formula \trf results in a formula $\psi$ that the follower states after the transition have to satisfy.
\begin{mydef}[Strongest Postcondition]
	Given a first-order logic formula $\varphi$ and transition formula \trf over the set of variables $V$, the strongest postcondition $	sp(\varphi, \trf)$ is the formula defined as:
	\begin{equation*}
		sp(\varphi, \trf) = (\exists V. \varphi \land \pi)[V' \mapsto V]
	\end{equation*}
\end{mydef}
\section{Program Safety}
This section serves as an introduction to program safety. Our goal is to prove whether a program is \textsl{correct} or to find a counterexample to correctness. In the following we define program safety, model a program's control-flow, and introduce traces.
\subsection{Control-Flow Graph}
 Assume we are given a program \prg, the order in which the program's instructions can be executed is called control-flow. We can construct a directed graph that models this control-flow in form of a control-flow graph.
 
 \begin{mydef}[Control-Flow Graph]
 	Given a program \prg, a control-flow graph $G = (Loc, \Pi, \delta, src, tgt, \loc{init})$ is a directed graph consisting of a finite set of program locations $Loc$, a set of transition formulas $\Pi$, a set of edges $\delta \subseteq Loc \times \Pi \times Loc$ between two locations, labeled with a transition formula, a function $src: \Pi \rightarrow Loc$ mapping the source of each transition formula to a location, a function $tgt: \Pi \rightarrow Loc$ mapping the target of transition formula to locations, and an initial location $\loc{init}$. 
 \end{mydef}
To illustrate the notion of control-flow graphs, assume we are given program $P$ as seen in Figure \ref{codeNoAss}, we construct the control-flow graph $G_P$ seen in Figure \ref{cfg:P:Noass}.
.\begin{center}
	\begin{minipage}[b]{0.4\linewidth}
		\begin{figure}[H]
			\centering
			\input{fig/lst_ex_p5.tex}
			\caption{Program $P$.}
			\label{codeNoAss}
		\end{figure}
	\end{minipage}
	\hfill
	\begin{minipage}[b]{0.59\linewidth}
		\begin{figure}[H]
			\centering
			\input{fig/fig_ex_p5_cfg.tex}
			\caption{Control-flow graph $G_P$ for program $P$.}
			\label{cfg:P:Noass}
		\end{figure}
	\end{minipage}
\end{center}
For defining correctness of a program, we need to extend the definition of control-flow graphs, by introducing error locations. An error location signifies an unwanted program state. We construct error locations by using the \texttt{assert} statement in program code. An assertion verifies whether a given expression holds, if so, the program continues normally, if not, it terminates, returning that an error location has been reached. We extend control-flow graphs to control-flow graphs with error locations.
 \begin{mydef}[Control-Flow Graph with Error Locations]
	Given a program \\ \prg, a control-flow graph with error location \cfg is a control-flow graph extended by an error locations $\loc{err}$.
\end{mydef}
For example, when we extend the program $P$ seen in Figure \ref{codeNoAss} with an \texttt{assert} statement as seen in Figure \ref{codeWithAss}, we get the control-flow graph depicted in Figure \ref{cfg:P:Ass}. We see the error location $\loc{err}$ which is reached by violating the expression \texttt{x == 22}.
\begin{center}
	\begin{minipage}[b]{0.4\linewidth}
		\begin{figure}[H]
			\centering
			\input{fig/lst_ex_p0.tex}
			\caption{Program $P$ with \\ assertion \texttt{x == 22}.}
			\label{codeWithAss}
		\end{figure}
	\end{minipage}
	\hfill
	\begin{minipage}[b]{0.59\linewidth}
		\begin{figure}[H]
			\centering
			\input{fig/fig_ex_p0_cfg.tex}
			\caption{Control-flow graph $G_P$ with error location $\loc{err}$ for program $P$.}
			\label{cfg:P:Ass}
		\end{figure}
	\end{minipage}
\end{center}
For the rest of the thesis, we denote control-flow graphs with error locations as control-flow graphs unless stated otherwise.
\subsection{Reachability}
The control-flow of a program can now be described by so called traces through the control-flow graph.
\begin{mydef}[Program Trace]
	Given a program	\prg, and its control-flow graph \\ G = \cfg, a trace $\tau = \trf_1, \trf_2, \ldots $ of a program is a finite or infinite sequence of transition formulas $\trf_1, \trf_2, \ldots$ with $\trf_i \in \Pi$, and $(scr(\trf_i), \trf_i, tgt(\trf_i)) \in \delta$
\end{mydef}
This definition only establishes that traces adhere to the graph structure of the control-flow graph. We need to set which traces are actually feasible.
\begin{mydef}[Feasible Traces]
	Given a trace $\tau = \trf_1, \trf_2, \ldots $ of a given program $\prg$, we call trace $\tau$ feasible, if for all transitions $\pi_i$ holds that $sp(\varphi_{i-1}, \pi_i) \neq \bot$. The formula $\varphi_i$ is recursively defined as: 
	\begin{align*}
		\varphi_i = 
		\begin{cases}
			\textbf{$\top$}, & \text{ for } i = 0 \\
			\textbf{$sp(\varphi_{i-1}, \pi_i)$}, & \text{ for a transition formula $\pi_i \in \tau$ }
		\end{cases}
	\end{align*}
\end{mydef}
Given trace $\tau = \pi_1, \ldots$ if there is a transition $\pi_{\bot}$ such that $sp(\varphi_{i-1}, \pi_{\bot}) = \bot$ holds, we call $\tau$ infeasible. We can now introduce the notion of reachability of program locations.
\begin{comment}
	\begin{mydef}[Reachability]
	Given a control-flow graph $\cfg$, we introduce a map $reach: Loc \mapsto \{\varphi\}$ that maps each program location $\ell_i \in Loc$ to a set of states, defined by first order logic formula $\varphi_i$, that are reachable from $\ell_i$. The set of states $\{\varphi_i\}$ is defined as the union $\bigcup\limits_{}^{\tau_j} sp(\varphi_{k-1}, \pi_k)$ for given traces $\tau_j = \pi_1, \ldots, \pi_k$ where $tgt(\pi_k) = \ell_i$. A program location $\ell_i$ is called reachable, if $reach(\ell_i) \neq \{\bot\}$, otherwise $\ell_i$ is called unreachable.
	\end{mydef}
\end{comment}
\begin{mydef}[Reachability using Floyd-Hoare Annotations]
	Given a control-flow graph $\cfg$, we introduce a map $reach: Loc \rightarrow \{\varphi\}$ that annotates each program location $\ell_i \in Loc$ with a set of states $\{\varphi_i\}$, defined by first order logic formula $\varphi_i$, this annotation is called Floyd-Hoare annotation. Such that for all edges $(\ell_i, \trf_i, \ell_j) \in \delta$ it holds that $\{sp(\trf_i, \varphi_i)\} \subseteq \{\varphi_j\}$, with $\varphi_j$ being the annotation of $\ell_j$.
	 A program location $\ell_i$ is called reachable, if $reach(\ell_i) \neq \{\bot\}$, otherwise $\ell_i$ is called unreachable.
\end{mydef}
A program is called safe if and only if every error location in a program's control-flow graph is unreachable. It is called unsafe if there is a reachable error location.

\section{Trace Abstraction}
To prove safety of a given program we employ trace abstraction \cite{10.1007/978-3-642-03237-0_7, 10.1007/978-3-642-39799-8_2, 10.1145/1706299.1706353}, which is an automata-theoretic approach used to prove reachability of error locations. This section will give an introduction to the approach and describe how the scheme works. \par

Given a program $P$ and its control-flow graph $\cfg$, the control-flow graph can be interpreted as a non-deterministic finite automaton \\ $\mathcal{A_P} = (Q, \Sigma, \Delta, Q_{init}, F)$ , with set of states $Q = Loc$, $q_{init} = \loc{init}$ being the initial state, accepting states $F = \loc{err}$ being the error state, alphabet $\Sigma = \Pi$ being the set of transition formulas, and $\Delta \subseteq Q \times \Sigma \times Q$ being the transition relation, derived from $\delta$. \par 
Automata can be extended to a Floyd-Hoare automaton by computing a Floyd-Hoare annotation $\beta$. This annotation is defined analogously to control-flow graph's annotations.
\begin{mydef}[Floyd-Hoare Automata]
	A Floyd-Hoare automaton \\$\mathcal{A} = (Q, \Sigma, \Delta, q_{init}, F, \beta)$ is a nondeterministic automaton consisting of, a set of states $Q$, an alphabet of symbols $\Sigma$, a transition relation $\Delta \subseteq Q \times \Sigma \times Q$, an initial state $q_{init}$, accepting states $F$, and a Floyd-Hoare annotation $\beta: Q \rightarrow \varphi$, where $\varphi$ is a first-order logic formula.
\end{mydef}
 It holds that $\beta(q) = \top$ if $q = q_{init}$ and $\beta(q) = \bot$ if $q \in F$. Therefore, every trace that is accepted by a Floyd-Hoare automaton is infeasible. \par
Given a program $P$, trace abstraction considers $P$'s control-flow graph $G_P$ as a nondeterministic finite automaton \cfgAutomat{P}. The automaton \\ \cfgAutomat{P} is used to iteratively compute a Floyd-Hoare automaton \\ \cfgAutomatHoare{D} with its language consisting of infeasible traces. Trace abstraction implements the paradigm shown in Figure \ref{traceAbstractionScheme}.

\begin{figure}[H]
	\centering
	\input{fig/fig_traceabstraction_schema.tex}
	\caption{The trace abstraction scheme.}
	\label{traceAbstractionScheme}
\end{figure}

We give a program $P$'s control-flow graph $G_P$  as input, which is interpreted as a non-deterministic automaton $\mathcal{A}_P$. The Floyd-Hoare automaton $\mathcal{A}_D$ is initialized as the empty set of traces. Then the following steps are repeated: 
\begin{enumerate}
	\item Construct a trace $\tau = \pi_1, \ldots, \pi_n$ and decide its feasibility. If $\tau$ is feasible, then we found an actual counterexample, returning that the $P$ is unsafe.
	\item If $\tau$ is proven infeasible, we construct a new automaton for that specific trace and compute a Floyd-Hoare annotation for it. This automaton now only accepts $\tau$, we can, however, model more traces by generalizing the automaton: We firstly add transitions $\pi_i \in \Sigma$ for which $\{sp(\trf_i, \varphi_i)\} \subseteq \{\varphi_j\}$ for annotations $\varphi_i$ and $\varphi_j$. Consequently, we merge states sharing the same annotation. This generalization is done in $\texttt{abstract}_F(\tau)$.
	\item We update the traces of the Floyd-Hoare automaton $\mathcal{A}_D$ by adding traces of the abstraction. The algorithm begins anew, by computing the set of remaining traces for which infeasibility has not yet been proven. If there are no traces left, we know that all traces to the error location are infeasible, terminating the algorithm, which returns that $P$ is safe.
\end{enumerate}
There are several additional methods of trace generalization, in this thesis we present a technique using loop summaries, in accelerated interpolation. \\ \par

To illustrate the trace abstraction scheme we demonstrate one iteration in the following. Assume we are given program $P$ and its control-flow graph \\ $G_P = \cfg$, as depicted in Figure \ref{codeWithAss} and Figure \ref{cfg:P:Ass}. The automaton $\cfgAutomat{P}$, with states $Q = Loc$, transition relation $\Delta = \delta$, initial state $Q_{init} = \ell_\init$, accepting states $F = \loc{err}$ is defined over alphabet: 
\begin{eqnarray} \Sigma = \Pi = \{ &\st{x:=0}, \st{y:=2}, \st{z:=3}, \st{x>20}, \st{x==22}, \st{x!=22}, \st{x<=20}, \\ &\st{x<=10}, \st{z:=x}, \st{x:=x+y}, \st{y:=y+1}, \st{x:=x+2}, \st{y:=y-3}\} 
\end{eqnarray} 
From $G_P$, we extract the trace:
\begin{eqnarray*}
	\tau_1= \st{x:=0;y:=2;z:=3}, \st{x<=20}, \st{x<=10}, \st{z:=x;x:=x+y;y:=y+1}, \st{x>20}, \st{x!=22}
\end{eqnarray*}
We consider $\tau_1$ as automaton $\mathcal{A}_1$ as seen in Figure \ref{traceNoAnnot}. From $\tau_1$ we compute a Floyd-Hoare annotation, by labeling $q_1$ with $\top$, and repeatedly applying the strongest post condition, as depicted in Figure \ref{traceAnnot}.\\
\begin{minipage}[t]{0.5\linewidth}
	\begin{figure}[H]
		\begin{centering}
		\input{fig/fig_trace_example_no_loop.tex}
		\caption{The trace $\tau_1$.}
		\label{traceNoAnnot}
		\end{centering}
	\end{figure}
\end{minipage}
	\begin{minipage}[t]{0.4\linewidth}
		\begin{figure}[H]
		\begin{centering}
		\input{fig/fig_trace_example_no_loop_hoare_annot.tex}
		\caption{Trace $\tau_1$ with Floyd-Hoare annotation $\beta$.}
		\label{traceAnnot}
		\end{centering}
		\end{figure}
	\end{minipage}

\begin{figure}[H]
	\begin{centering}
	\input{fig/fig_trace_example_no_loop_hoare_annot_generalized.tex}
	\caption{The abstracted trace of $\tau_1$.}
	\label{traceAnnotGeneral}
	\end{centering}
\end{figure}



We observe that the error state $q_7$ is labeled with $\bot$ proving that this trace is infeasible. This annotation only proves that the specific trace $\tau_1$ is infeasible, to exclude more that one trace at a time, we construct an \textsl{abstraction} of $\tau$. We firstly add transitions $ \pi_i \in \Sigma$ to each state $q_i$ with annotation $\varphi_i$, such that $sp(\varphi_i, \pi_i) \subseteq \{\varphi_i\}$ holds. After the addition of new transitions, we merge states with the same annotation, resulting in the abstracted trace as illustrated in Figure \ref{traceAnnotGeneral}. We extend the Floyd-Hoare automaton $\mathcal{A}_D$ by $\mathcal{A}_1$.


