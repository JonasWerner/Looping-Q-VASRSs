\newcommand{\stateSpace}{\ensuremath{S_{V, \mu}}}
\begin{comment}
	This chapter is mostly focused on trace abstraction $\rightarrow$  It introduces the reader to the concept of trace abstraction. \\
	- Introduce logic, logical variables, terms, formulas, transition formulas with primed and unprimed variables, programs, program states, loops $\rightarrow$  then program-, error traces, feasible and infeasible counterexamples, CFGs, interpolants. \\ - From intuitive to true definitions. \\
	Here the running example from the introduction gets dissected to illustrate the definitions. \\ 
	Further the problems loops can cause are introduced, followed by a definition of loop summaries $\rightarrow$ introduction reflexive transitive closure of a formula 
	15 pages
\end{comment}

This chapter will introduce mathematical background, our understanding and notation of logic and formulas, programs, control-flow, and other needed background definitions. Furthermore, we will give an overview of the \traceabstraction \cite{10.1007/978-3-642-03237-0_7} counterexample-guided abstraction refinement scheme used in \ultimate.

\subsection{Mathematical Background}
In this thesis we utilize various concepts of linear algebra, such as vector spaces, vector space bases, and more. This section will introduce important notations and definitions.
We define a \textsl{vector} as $\vec{x} = x_1, \ldots, x_n$ that consists of $n$ entries, $n$ is called the dimension of the vector. The vector containing only zeroes is denoted as $\vec{0}$, the vector containing only ones by $\vec{1}$. \\ \par We denote a \textsl{matrix} as
$S = \begin{bmatrix}
	s_{1 ,1} & \ldots & s_{1, n} \\
	\vdots & \ddots & \vdots \\
	s_{m ,1} & \ldots & s_{m, n} \\
\end{bmatrix}
$
with entries $s_{1, 1}, \ldots, s_{m, n}$. The integers $n$ and $m$ denote the number of columns and the number of rows respectively. A vector can be understood as a matrix of dimensions $1 \times n$, called row vector, or as a matrix of dimension $n \times 1$, called column vector.
We define the following operations on vectors and matrices.
\begin{mydef}[Scalar Multiplication]
	Given a scalar $a$ and an $m \times n$ matrix \\ $S = 
	\begin{bmatrix}
		a_{11} & \ldots & a_{1n} \\
		\vdots & \ddots & \vdots \\
		a_{m1} & \ldots & a_{mn} \\
	\end{bmatrix}$, the scalar multiplication $aS =
	\begin{bmatrix}
		c_{11} & \ldots & c_{1p} \\
		\vdots & \ddots & \vdots \\
		c_{m1} & \ldots & c_{mp} \\
	\end{bmatrix}$ produces an $m \times n$ matrix with entries \[c_{ij} =  a \cdot a_{ij} \] for $1 \leq i \leq m$ and $1 \leq j \leq p$.
\end{mydef}
\begin{mydef}[Matrix Multiplication]
	Given the $m \times n$ matrix $A = 
	\begin{bmatrix}
		a_{11} & \ldots & a_{1n} \\
		\vdots & \ddots & \vdots \\
		a_{m1} & \ldots & a_{mn} \\
	\end{bmatrix}$, and the $n \times p$ matrix $B = 
	\begin{bmatrix}
	b_{11} & \ldots & b_{1p} \\
	\vdots & \ddots & \vdots \\
	b_{n1} & \ldots & b_{np} \\
\end{bmatrix}$, the matrix product $AB = 
	\begin{bmatrix}
	c_{11} & \ldots & c_{1p} \\
	\vdots & \ddots & \vdots \\
	c_{m1} & \ldots & c_{mp} \\
	\end{bmatrix}$ produces the $m \times p$ matrix $AB$ with entries \[c_{i,j} = \sum_{k=1}^n a_{ik}\cdot b_{kj}\] for $1 \leq i \leq m$ and $1 \leq j \leq p$.
\end{mydef}
\begin{mydef}[Hadamard Product]
	Given matrices of same dimensions $m \times n$ $A = 
	\begin{bmatrix}
		a_{1 ,1} & \ldots & a_{1, n} \\
		\vdots & \ddots & \vdots \\
		a_{m ,1} & \ldots & a_{m, n} \\
	\end{bmatrix}$, and $B = 
	\begin{bmatrix}
		b_{1 ,1} & \ldots & b_{1, n} \\
		\vdots & \ddots & \vdots \\
		b_{m ,1} & \ldots & b_{m, n} \\
	\end{bmatrix}$, \\ the hadamard product $A*B = 
	\begin{bmatrix}
		c_{1 ,1} & \ldots & c_{1, p} \\
		\vdots & \ddots & \vdots \\
		c_{m ,1} & \ldots & c_{m, p} \\
	\end{bmatrix}$  produces an $m \times n$ matrix with entries $c_{i,j} = a_{ij} \cdot b_{ij}$ for $1 \leq i \leq m$ and $1 \leq j \leq p$.
\end{mydef}
\begin{mydef}[Linear Dependence]
	The sequence of vectors $\vec{x_1}, \ldots, \vec{x_n}$ of same dimension $d$, is linear independent, if and only if there exists no non zero scalars $a_1, \ldots, a_d$ such that $a_1\vec{x_1} + \ldots + a_n\vec{x_n} = \vec{0}$
\end{mydef}
\begin{mydef}[Vector Spaces]
	The set of vectors $S = \{\vec{x_1}, \ldots, \vec{x_n}\}$ is called a vector space.
\end{mydef}
\begin{mydef}[Linear Combination]
	Given a vector space $S = S = \{\vec{x_1}, \ldots, \vec{x_n}\}$ and a sequence of scalars $a_1, \ldots, a_n$, a linear combination of vectors and scalars is the sum $a_1\vec{x_1} + \ldots + a_n\vec{n}$
\end{mydef}
\begin{mydef}[Vector Space Basis]
	Given a vector space $S$, a set of vectors $B \subseteq S$ is a basis for vector space $S$ if its elements are linearly independent and every element of $S$ is a linear combination of elements of B.
\end{mydef}


\subsection{Logical Background}
To represent programs formally, we make use of first-order logic. This chapter will introduce our definitions and notations used throughout this thesis.

\subsubsection{Notation}
We utilize the standard logical notations: We represent boolean values as $\bot$, meaning \textsl{false}, and $\top$, meaning \textsl{true}. \\ Logical connectives are defined as:
\begin{itemize}
	\item Negation (\textsl{not}) denoted by: $\neg$
	\item Conjunction (\textsl{and}) denoted by: $\land$
	\item Disjunction (\textsl{or}) denoted by: $\lor$
	\item Implication (\textsl{if $\ldots$ then}) denoted by: $\rightarrow$
	\item Biconditional (\textsl{if and only if}) denoted by: $\leftrightarrow$
\end{itemize}
Formulas can be quantified, we define quantifiers as:
\begin{itemize}
	\item Existential quantification (\textsl{there exists}) denoted by: $\exists$
	\item Universal quantification (\textsl{for all}) denoted by: $\forall$
\end{itemize}

\subsubsection{Syntax}
We firstly introduce our first-order logic syntax.
Let $V = (\vocab{Var}, \vocab{Const}, \vocab{Fun}, \vocab{Pred})$ be a vocabulary consisting of the countable sets:
\begin{itemize}
	\item $\vocab{Var}$ containing all \textsl{variables}
	\item $\vocab{Const}$ containing all \textsl{constant symbols}
	\item $\vocab{Fun} $ containing all \textsl{function symbols}. Each symbol $f \in \vocab{Fun}$ has a natural number $\geq 1$ called arity of $f$
 	\item $\vocab{Pred}$ containing all \textsl{predicate symbols}. Each symbol $p \in \vocab{Pred}$ has a natural number $\geq 0$ called arity of $p$
\end{itemize}
Assume we are given such a vocabulary, we can construct first-order logic terms using the symbols in the vocabulary.
\begin{mydef}[Term] 
	Given a vocabulary $V = (\vocab{Var}, \vocab{Const}, \vocab{Fun}, \vocab{Pred})$, we define terms inductively as follows:
	\begin{itemize}
		\item Every $x \in \vocab{Var}$ is a term.
		\item Every $c \in \vocab{Const}$ is a term.
		\item If $t_0, \ldots, t_n$ are terms and $f \in \vocab{Fun}$ being a function symbol with arity $n$, then $f(t_0, \ldots, t_n)$ is a term.
	\end{itemize}
\end{mydef}
Using first-order logic terms, we can introduce first-order logic formulas.
\begin{mydef}[Formula]
	Given vocabulary $V = (\vocab{Var}, \vocab{Const}, \vocab{Fun}, \vocab{Pred})$, first-order logic formulas are inductively defined as follows:
	\begin{itemize}
		\item $\bot$ is a formula.
		\item If  $t_0, \ldots, t_n$ are terms, and $p \in \vocab{Pred}$ is a predicate symbol with arity $n$, \\ then $p(t_0, \ldots, t_n)$ is a formula.
		\item If $\varphi$ is a formula, then $\neg \varphi$ is a formula.
		\item If $\varphi$ and $\psi$ are formulas, then $\varphi \land \psi$ are formulas.
		\item If $\varphi$ is a formula, and $x \in \vocab{Var}$ then $\exists x. \varphi$ is a formula.
	\end{itemize}
\end{mydef}
To give variables, constants, functions, and predicates concrete values, we can assign them a model.
\begin{mydef}[Model]
	Given vocabulary $V = (\vocab{Var}, \vocab{Const}, \vocab{Fun}, \vocab{Pred})$, a model $\mathcal{M} = (D, \interpret)$ is a tuple consisting of a nonempty set $D$, called interpretation domain, and an interpretation function \interpret that assigns constants, functions, and predicates over $D$ to symbols in $V$. $M$ has the following characteristics:
	\begin{itemize}
		\item The domain of \interpret is $\vocab{Const} \cup \vocab{Fun} \cup \vocab{Pred}$
		\item \interpret maps every constant symbol $c \in \vocab{Const}$ to an element in $D$
		\item \interpret maps every function symbol $f \in \vocab{Fun}$, with arity $n$, to a corresponding n-ary function with domain $D^n$ and range $D$
		\item \interpret maps every predicate symbol $p \in \vocab{Pred}$ with arity $n$ to an n-ary relation over the domain $D$
	\end{itemize}
\end{mydef}
Using a model, we can now assign concrete values to variables.
\begin{mydef}[Assignment of Variables]
	Given vocabulary \\c $V = (\vocab{Var}, \vocab{Const}, \vocab{Fun}, \vocab{Pred})$, and model $\mathcal{M} = (D, \interpret)$, an assignment of variable $v \in \vocab{Var}$ is a function $\rho: v \rightarrow D$. Mapping each variable a value in domain $D$.
\end{mydef}
Assume $f \in \vocab{Fun}$ is a function defined as $f: X \mapsto Y$ with some domain $X$ and range $Y$. Let $x \in X$ and $y \in Y$, we use $f[x \mapsto y]$  to denote the function that maps all $\bar{x} \in X \backslash \{ x \}$ to $f(\bar{x})$ and $x$ to $y$.

\subsubsection{Semantics}
Assume we are given a vocabulary $V = (\vocab{Var}, \vocab{Const}, \vocab{Fun}, \vocab{Pred})$, we know how to assign values using models and variable assignments. The task now is to understand how to interpret them. This section serves to introduce semantics of fist-order logic.
\begin{mydef}[Evaluation of Terms]
	Let $V = (\vocab{Var}, \vocab{Const}, \vocab{Fun}, \vocab{Pred})$ be a vocabulary, $\mathcal{M} = (D, \interpret)$ a model, and $\rho$ a variable assignment. The evaluation of terms is a function $\eval{\cdot}$ that is inductively defined as:
	\begin{itemize}
		\item For each $v \in \vocab{Var}$, $\eval{v} = \rho(v)$
		\item For each $c \in \vocab{Const}$, $\eval{c} = \interpret(c)$
		\item If $t_0, \ldots, t_n$ are terms, $f \in \vocab{Fun}$, with f having arity $n$ then \\ $\eval{f(t_0, \cdots, t_n)} = \interpret(f)(\eval{t_0}, \ldots, \eval{t_n})$
	\end{itemize}
\end{mydef}
From the evaluation of terms we can derive the evaluation of formulas, which decides whether a formula is \textsl{true} or \textsl{false}.

\begin{mydef}[Evaluation of Formulas]
		Let $V = (\vocab{Var}, \vocab{Const}, \vocab{Fun}, \vocab{Pred})$ be a vocabulary, $\mathcal{M} = (D, \interpret)$ a model, a variable assignment $\rho$, and $\varphi_0, \varphi_1$ being first-order logic formulas over $V$. The evaluation of formulas is a function $\eval{\cdot}$ that is inductively defined as: \\
		\begin{itemize}
			\item {\makebox[3.5cm]{$\eval{\bot} = \hfill$}} = false
			\item {\makebox[3.5cm]{$ \eval{p(t_0, \ldots, t_n)} \hfill$}} =
				$
				\begin{cases}
					true, & \text{for } (\eval{t_0}, \ldots, \eval{t_n}) \in \interpret(p)\\
					false, & \text{otherwise}
				\end{cases}
				$
			\item {\makebox[3.5cm]{$\eval{\neg \varphi_0} \hfill$}} =
				$
				\begin{cases}
					true & \text{for } \eval{\varphi_0} \text{ \textbf{false}}\\
					false, & \text{for } \eval{\varphi_0} \text{ \textbf{true}}\\
				\end{cases}
				$
			\item {\makebox[3.5cm]{$\eval{\varphi_0 \land \varphi_1} \hfill$}} =
				$
				\begin{cases}
					true, & \text{for } \eval{\varphi_0} \text{ \textbf{true} and } \eval{\varphi_1} \text{true} \\
					false, & \text{otherwise} \\
				\end{cases}
				$
			\item {\makebox[3.5cm]{$\eval{\exists v. \varphi_0} \hfill$}} =
				$
				\begin{cases}
					true, & \text{if there exists } x \in D \text{ where } \eval{\varphi_0 [v \mapsto x]} \text{ true} \\
					false, & \text{otherwise} \\
				\end{cases}
				$
		\end{itemize}
\end{mydef}
A formula $\varphi$ is called satisfiable if there exists a model $\mathcal{M} = (D, \interpret)$ and a variable assignment $\rho$ such that $\eval{\varphi}$ is true. A formula is valid if there is no model $\mathcal{M} = (D, \interpret)$, such $\eval{\varphi}$ is false.
\subsection{Programs}
In this chapter we will introduce our understanding on how to model programs using first-order logic. We will begin with our syntax of programs, then explain program statement semantics.

\subsubsection{Program Syntax}
Assume we are given the example program as seen in Figure \ref{codeNoAss}. We can see that the code contains various typed variables such as \texttt{x:int} and instructions over these variables. \\ These instructions use the following context-free grammar $\Sigma$ that is a derived and simplified version of the grammar of the intermediate verification language Boogie\cite{Boogie}.
\setlength{\grammarparsep}{20pt plus 1pt minus 1pt} % increase separation between rules
\setlength{\grammarindent}{12em} % increase separation between LHS/RHS
\begin{figure}[H]
	\input{fig/grammar.tex}
	\caption{Context-free grammar $\Sigma$ detailing program instructions.}
	\label{grmr}
\end{figure}
We call words derived from $\langle Stmt \rangle$ program statements, words derived from $\langle Expr \rangle$ expressions, and $v \in V$ represents a program variable or a constant in a fitting model.
We define programs as follows:

\begin{mydef}[Programs]
	A program is a triple \prg, with $V$ being a set of variables, a function $\mu: V \rightarrow \{ \mathbb{Z}, \mathbb{R}, \mathbb{B} \}$ that maps variables $v \in V$ to a domain, which is either the set of integers, the rational numbers, or boolean values $\mathbb{B} = \{\textbf{true}, \textbf{false}\}$. Furthermore, $st$ is a derived word from $\Sigma$ representing the program instructions.
\end{mydef}
\subsubsection{Program Semantics}
We consider five kinds of program statements in the grammar $\Sigma$: An assumption over variables \texttt{assume}, an assignment to variables \texttt{:=}, a non deterministic assignment \texttt{havoc}, a program loop \texttt{while}, that repeats a statement until its loop guard, the $\langle WildcardExpr \rangle$, no longer holds, and an conditional branching, in form of an \texttt{if else} statement. With the first three: \st{assume $\langle Expr \rangle$ }, \st{v := $\langle Expr \rangle$}, and \st{havoc v}, being atomic statements. Whereas \texttt{while} and \texttt{if else} are a compound of atomic statements.
We regard expressions $\langle Expr \rangle$ as logical formulas. Most program statements change the assignment of variables and with that changes the state a program is in. 
\begin{mydef}[Program State]
	Given a program \prg a program state is a function $\sigma: V \rightarrow \mu(V)$ that assigns each variable $v \in V$ a value in its domain $\mu(v)$. We denote the set of all program states as \stateSpace.
\end{mydef}
We use fist-order logic formulas to detail sets of states.
\begin{mydef}[Sets of Program States]
	Given a first-order logic formula $\varphi$, a set of program states $\{ \varphi \}$ contains all program states $\sigma_i: V \rightarrow \mu_i(V)$ for which $\eval{\varphi [V\mapsto \mu_i(V) ]}$ is true.
\end{mydef}
The set of all program states $\stateSpace$ is defined as $\{\top\}$ and the empty set of program states as $\{\bot\}$.
Using program states, we can now set the semantics for the three atomic program statements.
\begin{mydef}[Program Statement Semantics]
		Given a program \prg, we define the semantics of program statements as a binary relation over program states $\sigma_1, \sigma_2$, whereas $\sigma_1$ denotes the program's state before the statement and $\sigma_2$ after. To model time steps, the variables in $\sigma_2$ are replaced by so called primed variables, meaning every variable $v \in Var$ is replaced by $v'$. \\
	\begin{itemize}
		\item The semantics of the \textnormal{\texttt{assumption}} statement: \textnormal{\st{assume $\langle Expr \rangle$}} are defined as the relation: \\
		\begin{equation*}
			\{(\sigma_1, \sigma_2) \in \stateSpace \times \stateSpace\ |\ \eval{\langle Expr \rangle}\ \land\ \bigwedge\limits_{v \in V} v' = v  \text{ is true}\}
		\end{equation*}
		\item The semantics of the \textnormal{\texttt{assignment}} statement: \textnormal{\st{v := $\langle Expr \rangle$}} are defined as the relation: \\
		\begin{equation*}
			 \{(\sigma_1, \sigma_2) \in \stateSpace \times \stateSpace\ |\ \eval{x' = \eval{\langle Expr\rangle}\ \land\ \bigwedge\limits_{v \in V, v \not= x} v' = v 
			} \text{ is true}\}
		\end{equation*}
		\item The semantics of the \textnormal{\texttt{havoc}} statement: \textnormal{\st{havoc v}} are defined as the relation: \\
		\begin{equation*}
			\{(\sigma_1, \sigma_2) \in \stateSpace \times \stateSpace\ |\ \bigwedge\limits_{v 	\in V, v \not= x} v' = v 
			\text{ is true} \}
		\end{equation*}
	\end{itemize}
\end{mydef}


\begin{comment}
	\begin{mydef}[Sets of Program States]
	To represent multiple program states we use first-order logic formulas. Given a first-order logic formula $\varphi$, defined over variables in $V$, we denote $\{\varphi\} = \{s \in \stateSpace | \eval{\varphi} \text{ where } \rho = s\}$
	\end{mydef}
\end{comment}
To model while and if else, we can concatenate these three atomic statements to form sequences of statements.
\begin{mydef}[Concatenation of Program Statements]
	Let \st{$s_1$} and \st{$s_2$} be atomic program statements and $\sigma_1$, $\sigma_2$ be program states, the semantics of the concatenation \st{$s_1;s_2$} are defined as:
	\begin{equation*}
		\{(\sigma_1, \sigma_2) \in \stateSpace \times \stateSpace\ |\ \exists \sigma_3 \in \stateSpace. (\sigma_1, \sigma_3) \in \text{\st{$s_1$}} \land (\sigma_3, \sigma_2) \in \text{\st{$s_2$}}
	\end{equation*}
\end{mydef}

The change from one program state to another is called a \textsl{transition}. To model transitions efficiently, we introduce transition formulas.
\begin{mydef}[Transition Formula]
		Given a program \prg with variable $v \in V$, transitions from program state $\sigma_1$ to state $\sigma_2$ is characterized by the transition formula defined over variables $V$ and $V'$ whereas $V'$ contains all variables found in $V$ but primed. The set $V$ characterizes the variables before the transition, e.g. the variable valuations in $\sigma_1$, in contrast to $V'$ which represents the variable valuations after the transition. A transition between two states is only possible, if and only if the transition formula is satisfiable for the give states. The number of variables in $V$ is called the dimension of the transitions formula.
\end{mydef}
 We specify for each of the atomic program statements the following \textsl{transition formulas}: 
 \begin{center}
\begin{itemize}
	\item \st{assume $\langle Expr \rangle$}: 
	\begin{equation*}
		Expr\ \land \bigwedge\limits_{v \in V} v' = v 
	\end{equation*}
	\item \st{v := $\langle Expr \rangle$}:
	\begin{equation*}
		v' = Expr\ \land \bigwedge\limits_{v \in V, v \not= x} v' = v 
	\end{equation*}
	\item \st{havoc v}: 
	\begin{equation*}
			\bigwedge\limits_{v \in V, v \not= x} v' = v 
	\end{equation*}
\end{itemize} 
 \end{center}
For brevity's sake we will use the notation \st{$\langle Expr \rangle$} for assumptions, and omit $\bigwedge\limits_{v \in V} v' = v$ in assumptions and assignments. \\
With transition formulas we can model state transitions using the strongest postcondition. The strongest postcondition applied on a program state $\sigma$, that satisfies a given first-order logic formula $\varphi$, and a transition formula \trf results in a formula $\psi$ that the follower states after the transition have to satisfy.
\begin{mydef}[Strongest Postcondition]
	Given a first-order logic formula $\varphi$ and transition formula \trf with set of variables $V$, the strongest postcondition $	sp(\varphi, \trf)$ is the formula defined as:
	\begin{equation*}
		sp(\varphi, \trf) = (\exists V. \varphi \land \tf)[V' \mapsto V]
	\end{equation*}
\end{mydef}
\subsection{Program Safety}
This section serves as an introduction to our understanding of program verification. Our goal is to prove whether a program is \textsl{correct} or to find a counterexample to correctness. In the following we define our understanding of safety, model a program's control-flow, and introduce traces.
\subsubsection{Control-Flow Graph}
 Assume we are given a program \prg, the order in which the program's instructions can be executed is called control-flow. We can construct a directed graph that models this control-flow in form of a control-flow graph.
 
 \begin{mydef}[Control-Flow Graph]
 	Given a program \prg, a control-flow graph $G = (Loc, \Pi, \delta, src, tgt, \loc{init})$ is a directed graph consisting of a finite set of program locations $Loc$, a set of transition formulas $\Pi$, a set of edges $\delta \subseteq Loc \times \Pi \times Loc$ between two locations, labeled with a transition formula, a function $src: \Pi \rightarrow Loc$ mapping the source of each transition formula to a location, a function $tgt: \Pi \rightarrow Loc$ mapping the target of transition formula to locations, and an initial location $\loc{init}$. 
 \end{mydef}
To illustrate the notion of control-flow graphs, assume we are given program $P$ as seen in Figure \ref{codeNoAss}, we construct the control-flow graph $G_P$ seen in Figure \ref{cfg:P:Noass}
.\begin{center}
	\begin{minipage}[b]{0.4\linewidth}
		\begin{figure}[H]
			\centering
			\input{fig/lst_ex_p5.tex}
			\caption{Program $P$.}
			\label{codeNoAss}
		\end{figure}
	\end{minipage}
	\hfill
	\begin{minipage}[b]{0.59\linewidth}
		\begin{figure}[H]
			\centering
			\input{fig/fig_ex_p5_cfg.tex}
			\caption{Control-flow graph $G_P$ for program $P$.}
			\label{cfg:P:Noass}
		\end{figure}
	\end{minipage}
\end{center}
For defining correctness of a program, we need to extend the definition of control-flow graphs, by introducing error locations. An error location signifies an unwanted program state. We construct error locations by using the \texttt{assert} statement in program code. An assertion verifies whether a given expression holds, if so, the program continues normally, if not, it terminates, returning that an error location has been reached. We extend control-flow graphs to control-flow graphs with error locations.
 \begin{mydef}[Control-Flow Graph with Error Locations]
	Given a program \\ \prg, a control-flow graph with error location \cfg is a control-flow graph extended by a set of error locations $\loc{err}$.
\end{mydef}
For example, when we extend the program $P$ seen in Figure \ref{codeNoAss} with an \texttt{assert} statement as seen in Figure \ref{codeWithAss}, we get the control-flow graph depicted in Figure \ref{cfg:P:Ass}. We see the error location $\loc{err}$ which is reached by violating the expression \texttt{x == 22}.
\begin{center}
	\begin{minipage}[b]{0.4\linewidth}
		\begin{figure}[H]
			\centering
			\input{fig/lst_ex_p0.tex}
			\caption{Program $P$ with \\ assertion \texttt{x == 22}.}
			\label{codeWithAss}
		\end{figure}
	\end{minipage}
	\hfill
	\begin{minipage}[b]{0.59\linewidth}
		\begin{figure}[H]
			\centering
			\input{fig/fig_ex_p0_cfg.tex}
			\caption{Control-flow graph $G_P$ with error location $\loc{err}$ for program $P$.}
			\label{cfg:P:Ass}
		\end{figure}
	\end{minipage}
\end{center}
For the rest of the thesis, denote control-flow graph with error location as control-flow graph unless stated otherwise.
\subsubsection{Reachability}
The control-flow of a program can now be described by so called traces through the control-flow graph.
\begin{mydef}
	Given a program	\prg, its control-flow graph \\ G = \cfg, a trace $\tau = \trf_1, \trf_2, \ldots $ of a program is a finite or infinite sequence of transitions $\trf_1, \trf_2, \ldots$ with $\trf_i \in \Pi$, and $(scr(\trf_i), \trf_i, tgt(\trf_i)) \in \delta$
\end{mydef}
This definition only establishes that traces adhere to the graph structure of the control-flow graph. We need to set which traces are actually feasible.
\begin{mydef}
	Given a trace $\tau = \trf_1, \trf_2, \ldots $ of a program given program $\prg$, $\tau$ is called feasible, if for all transition $\pi_i$: $sp(\varphi_{i-1}, \pi_i) \neq \bot$, $\varphi_i$ is recursively defined as: 
	\begin{align*}
		\varphi_i = 
		\begin{cases}
			\textbf{$\top$}, & \text{ for } i = 0 \\
			\textbf{$sp(\varphi_{i-1}, \pi_i)$}, & \text{ for a transition formula $\pi_i \in \tau$ }
		\end{cases}
	\end{align*}
\end{mydef}
Given trace $\tau = \pi, \ldots$, if there is a transition $\pi_{\bot}$ where $sp(\varphi_{i-1}, \pi_{\bot}) = \bot$ in a trace $\tau$, $\tau$ is called infeasible. We can now introduce the notion of reachability of program locations.
\begin{mydef}
	Given a control-flow graph $\cfg$, we introduce a map $reach: Loc \mapsto \{\varphi\}$ that maps each program location $\ell_i \in Loc$ to a set of states, defined by first order logic formula $\varphi_i$, that are reachable from $\ell_i$. The set of states $\{\varphi_i\}$ is defined as the union $\bigcup\limits_{}^{\tau_j} sp(\varphi_{k-1}, \pi_k)$ for given traces $\tau_j = \pi_1, \ldots, \pi_k$ where $tgt(\pi_k) = \ell_i$. A program location $\ell_i$ is called reachable, if $reach(\ell_i) \neq \{\bot\}$, otherwise $\ell_i$ is called unreachable.
\end{mydef}
A program is called safe if and only if every error location in a program's control-flowgraph is unreachable. It is called unsafe if there is a reachable error location.

\subsection{Trace Abstraction}
To prove safety of a given program we employ trace abstraction \cite{10.1007/978-3-642-03237-0_7, 10.1007/978-3-642-39799-8_2, 10.1145/1706299.1706353}, which is an automata-theoretic approach used to prove reachability of error traces.

\begin{figure}[H]
	\centering
	\input{fig/fig_traceabstraction_schema.tex}
	\caption{Control-flow graph $G_{P_1}$ for program $P_1$.}
	\label{traceAbstractionScheme}
\end{figure}







